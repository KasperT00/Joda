{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Johdanto datatieteeseen\n",
    "\n",
    "**Oppimispäiväkirja**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Luentoviikko 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liiketoimintatiedon hallinnan kurssilla opimme liiketoimintaan liittyvän tiedon jalostamisesta ja sen kulusta organisaatiossa. Toisella luentoviikolla Johdanto datatieteeseen -kurssilla kävimme esimerkiksi ETL ja DAD-prosessit, joista ETL on aiemmin tuttu. Tässä prosessissa tietoa kerätään (Extract), muunnetaan oikeaan muotoon (Transform) ja ladataan eteenpäin tietovarastoihin (Load). DAD on prosessina samankaltainen, jossa tietoa kerätään ja haetaan, sekä jalostetaan, mutta tämän prosessin tahti on nopeampaa. Näiden eri prosessien avulla kerättyä dataa voidaan hyödyntää analyysissä ja edelleen päätöksenteossa. Itse ymmärsin myös ETL-prosessin tiedon olevan vielä jalostetumpaa, kuin DAD-prosessin tiedon. Tämän oppimispäiväkirjan kappaleen tekoon hyödynsin ensimmäistä luentoa, sekä useita eri stackoverflow tekstejä, sekä scrapy-raapijan dokumentaatio raapijan luomiseen.\n",
    "\n",
    "Erilaisia analyysimenetelmiä esiteltiin luennolla, jotka olivat ennestään tuttuja. Näitä olivat diagnostiivinen, deskriptiivinen, prediktiivinen ja preskriptiivinen unohtamatta eksploratiivista analytiikkaa. Vaikka nämä suommen kieleen väkisin väännetyt sanat kuulostavat vaikealta, on ne helppo muistaa niiden englanninkielen merkityksen kautta. Hauskaksi koin luennolla esitetyn nelikentän, jossa yksi sisällön aiheista oli \"Asiat, joita emme tiedä ja emme tiedä tietävämme\". Tämä on kuitenkin mielestäni yksi tärkeimmistä aiheista, sillä asian tiedostaminen on vaikeaa, jos ei tiedä tiedä, että ei tiedä. Itse en esimerkiksi tiennyt raapijoista ja ryömijöistä ennen tätä kurssia. \n",
    "\n",
    "Raavitaan dataa irti ja ryömitään datamassoissa tulee näistä ensimmäisenä mieleen. Opiskelijoistakin raavittiin jo heti kurssin alussa sen kulusta irti tietoa flingan avulla. Kurssin alussa olikin itselle haastavaa hahmottaa kaikki eri järjestelmät, jonka myös hiljaisesta tiedosta muutin sanalliseksi flingaan. Kehityskohteena olisikin esimerkiksi konkreettisesti luoda lista, jossa kerrotaan suoraan jäjestelmä, sen tarkoitus ja käyttö. Toiseen luentoviikkoon mennessä en vielä tiedä, tulisiko oppimispäivää kirjoittaa Jupyterillä vai esimerkiksi Wordilla. Päädyin, kuitenkin opiskelutoverini kanssa Jupyteriin varmuuden vuoksi. Itseäni harmittaa ettei Jupyterissä ole mahdollisuutta ajaa komentoja konsolin kautta, jonka kohta myös demonstroin. \n",
    "\n",
    "Eniten mieleen toisen luentoviikon luennolta jäi Scrapy mainituista raapijoista. Päätin täten perehtyä raapijoihin ja Scrapy kirjastoon. En kuitenkaan tiennyt, että kyseistä raapijaa käydään läpi myös demo-luennolla. Muutamien ja tusinan tuntien taistelun jälkeen sain asennettua ohjelmistot ja kirjastot, sekä luotua ensimmäisen raapijan. Sovelsin ohjelmistoa K-Ruoka sivustoon, jossa se kerää tietoa hedelmistä ja vihanneksista. Päätin yksinkertaisuuden vuoksi kerätä alkuun ainoastaan tietoa niiden nimistä. Alussa ongelmaksi koitui Jupyter, jossa raapijan pystyi ajamaan ainoastaan kerran, sen konsolin puutteen vuoksi, ennen kuin Kernel oli resetoitava. Useat sivustot myös suosittelivat Scrapy-raapijan suorittamista virtuaalisessa venv-ympäristössä. En tietenkään tätä ohjetta noudattanut, eikä siitä myöhemminkään ole ongelmaa koitunut, mutta aion varmasti siihen myöhemmin tutustua. Monia muitakin ongelmia oli matkan varrella, joista saisi muutaman sivun tekstin aikaiseksi. Alla on koodi kyseistä raapijasta, jonka nimesin syömäni ruuan mukaan \"Pasta\" -nimiseksi. Koodin toteuttaminen olisi varmasti ollut helpompaa vasta demo-session jälkeen, mutta koen kuitenkin oppineen huomattavasti enemmän tehtyäni sen itse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "class Pasta(scrapy.Spider):    \n",
    "    name = \"Pasta\"     \n",
    "    start_urls = [\n",
    "        'https://www.k-ruoka.fi/kauppa/tuotehaku/hedelmat-ja-vihannekset', \n",
    "    ]\n",
    "    def parse(self, response):     #Prosessoi ja palauttaa raavitun datan\n",
    "        for tuote in response.css('li.bundle-list-item'):  # Sivun tuotteiden osoite\n",
    "            texti = tuote.css('span::text').get()# Tämä on siellä tuotetiedon sisällä ja se on se teksti osa\n",
    "            yield {'tuote':texti} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yllä esitetty \"Pasta\"-ryömiijä voi kerätä tietoa sille annetuista osoitteista, mutta eri sivujen elementtien tiedot on sille kerrottava. Luomani ryömijä ei kuitenkaan pysty lukemaan esimerkiksi esimerkissä olevaa sivua kokonaan, sillä se on dynaaminen ja latautuu samalla sivua selatessa. Aion kuitenkin löytää tähän ratkaisun ja saada ryömijä toimimaan myös dynaamisilla sivuilla, sekä menemään seuraaville sivuille. \n",
    "\n",
    "Tällä luentoviikolla opin\n",
    "- Tiedon jalostamisen eri menetelmiä ja prosesseja\n",
    "- Tiedon analyysin käsitteistöä ja menetelmiä\n",
    "- Raapijat ja ryömijät tiedon hankinnassa\n",
    "- Luomaan yksinkertaisen scrapy-kirjastoon pohjautuvan ryömijän\n",
    "- Lukemaan dokumentaatioita kirjastoista\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        screen_height = self.driver.execute_script('return window.screen.height;')\n",
    "        i = 1\n",
    "        while True: # Täsä skrollataa sivua, että se latautuu\n",
    "            self.driver.execute_script(\"window.scrollTo(0, {screen_height}*{i})\".format(screen_height = screen_height, i=i))\n",
    "            i += 1\n",
    "            time.sleep(0.2)\n",
    "            scroll_height = self.driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "            if (screen_height) * i > scroll_height:\n",
    "                break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self):\n",
    "        self.driver = webdriver.Chrome('C:/Users/kaspe/Documents/Koulu/Muita/chromedriver_win32/chromedriver.exe')\n",
    "        self.driver.get('https://www.verkkokauppa.com/fi/catalog/22a/Puhelimet/products?pageNo=84')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        self.driver.execute_script(\"window.scrollBy(0, -2000)\")\n",
    "        time.sleep(1)\n",
    "        if self.driver.find_element_by_xpath('.//div/button[@aria-label=\"Seuraava sivu\"]'):\n",
    "            self.driver.find_element_by_xpath('.//div/button[@aria-label=\"Seuraava sivu\"]').click()\n",
    "            time.sleep(2)\n",
    "            currentURL = self.driver.current_url\n",
    "            yield scrapy.Request(currentURL, callback=self.parse)\n",
    "        else:\n",
    "            self.driver.quit()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
